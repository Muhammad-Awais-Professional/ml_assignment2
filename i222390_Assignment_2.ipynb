{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qhLc14KeMM0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n",
        "y = np.array([3, 5, 7, 9, 11], dtype=np.float32)\n",
        "\n",
        "def predict(X, w, b):\n",
        "    return w * X + b\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2) / 2\n",
        "\n",
        "def compute_gradients(X, y, w, b):\n",
        "    N = len(y)\n",
        "    y_pred = predict(X, w, b)\n",
        "    error = y - y_pred\n",
        "    grad_w = -np.sum(X * error) / N\n",
        "    grad_b = -np.sum(error) / N\n",
        "    return grad_w, grad_b\n",
        "\n",
        "def sgd_linear_regression(X, y, lr=0.01, epochs=100, batch_size=1, momentum=0.0, method='stochastic'):\n",
        "    w, b = 0.0, 0.0\n",
        "    velocity_w, velocity_b = 0.0, 0.0\n",
        "    loss_history = []\n",
        "    N = len(y)\n",
        "\n",
        "    if method == 'batch':\n",
        "        batch_size = N\n",
        "    elif method == 'stochastic':\n",
        "        batch_size = 1\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        indices = np.arange(N)\n",
        "        np.random.shuffle(indices)\n",
        "        X_shuffled = X[indices]\n",
        "        y_shuffled = y[indices]\n",
        "\n",
        "\n",
        "        for i in range(0, N, batch_size):\n",
        "            end = i + batch_size\n",
        "            X_batch = X_shuffled[i:end]\n",
        "            y_batch = y_shuffled[i:end]\n",
        "\n",
        "            grad_w, grad_b = compute_gradients(X_batch, y_batch, w, b)\n",
        "\n",
        "            velocity_w = momentum * velocity_w + lr * grad_w\n",
        "            velocity_b = momentum * velocity_b + lr * grad_b\n",
        "\n",
        "            w = w - velocity_w\n",
        "            b = b - velocity_b\n",
        "\n",
        "\n",
        "        y_pred = predict(X, w, b)\n",
        "        loss = mse_loss(y, y_pred)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "    return w, b, loss_history\n",
        "\n",
        "\n",
        "methods = ['stochastic', 'batch', 'mini-batch']\n",
        "results = {}\n",
        "epochs = 100\n",
        "\n",
        "print(\"Training with different SGD methods:\")\n",
        "for method in methods:\n",
        "    if method == 'mini-batch':\n",
        "        current_batch_size = 2\n",
        "    else:\n",
        "        current_batch_size = 1\n",
        "    w, b, loss_history = sgd_linear_regression(X, y, lr=0.01, epochs=epochs, batch_size=current_batch_size, momentum=0.0, method=method)\n",
        "    results[method] = {'w': w, 'b': b, 'loss_history': loss_history}\n",
        "    print(f\"Method: {method} --> w = {w:.4f}, b = {b:.4f}, final loss = {loss_history[-1]:.6f}\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "for method in methods:\n",
        "    plt.plot(results[method]['loss_history'], label=method)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.title(\"Convergence of Different SGD Methods\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "momentum_values = [0.0, 0.5, 0.9]\n",
        "exp_results = {}\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plot_idx = 1\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for momentum in momentum_values:\n",
        "        w, b, loss_history = sgd_linear_regression(X, y, lr=lr, epochs=epochs, batch_size=1, momentum=momentum, method='stochastic')\n",
        "        exp_results[(lr, momentum)] = {'w': w, 'b': b, 'loss_history': loss_history}\n",
        "        plt.subplot(len(learning_rates), len(momentum_values), plot_idx)\n",
        "        plt.plot(loss_history)\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.title(f\"lr={lr}, momentum={momentum}\")\n",
        "        plot_idx += 1\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ]
}